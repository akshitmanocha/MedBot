{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9747027,"sourceType":"datasetVersion","datasetId":5967091},{"sourceId":9750921,"sourceType":"datasetVersion","datasetId":5969907}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft --quiet","metadata":{"execution":{"iopub.status.busy":"2024-10-29T06:41:59.430091Z","iopub.execute_input":"2024-10-29T06:41:59.430383Z","iopub.status.idle":"2024-10-29T06:42:14.194440Z","shell.execute_reply.started":"2024-10-29T06:41:59.430350Z","shell.execute_reply":"2024-10-29T06:42:14.193309Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n\n# Set your API token as an environment variable\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"  \n\n# Login using the environment variable\nlogin(token=os.environ[\"HUGGINGFACE_TOKEN\"], write_permission=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T06:42:17.855676Z","iopub.execute_input":"2024-10-29T06:42:17.856451Z","iopub.status.idle":"2024-10-29T06:42:18.636964Z","shell.execute_reply.started":"2024-10-29T06:42:17.856397Z","shell.execute_reply":"2024-10-29T06:42:18.635954Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preparation and Tokenization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, LlamaForCausalLM, Trainer, TrainingArguments\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import Dataset\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-28T17:41:28.842327Z","iopub.execute_input":"2024-10-28T17:41:28.842702Z","iopub.status.idle":"2024-10-28T17:41:47.674091Z","shell.execute_reply.started":"2024-10-28T17:41:28.842666Z","shell.execute_reply":"2024-10-28T17:41:47.673287Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/medquad/medquad.csv')\ndata.drop(columns=['source'],inplace=True)\n\n# Lets define a fuction to combine the question answer and focus area column\ndef conc(data):\n    return f\"Queston: {data['question']} Context: {data['focus_area']} Answer: {data['answer']}\"\n    \ndata['text'] = data.apply(conc,axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:41:47.675905Z","iopub.execute_input":"2024-10-28T17:41:47.676532Z","iopub.status.idle":"2024-10-28T17:41:48.445363Z","shell.execute_reply.started":"2024-10-28T17:41:47.676496Z","shell.execute_reply":"2024-10-28T17:41:48.444367Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer focus_area  \\\n0  Glaucoma is a group of diseases that can damag...   Glaucoma   \n1  Nearly 2.7 million people have glaucoma, a lea...   Glaucoma   \n2  Symptoms of Glaucoma  Glaucoma can develop in ...   Glaucoma   \n3  Although open-angle glaucoma cannot be cured, ...   Glaucoma   \n4  Glaucoma is a group of diseases that can damag...   Glaucoma   \n\n                                                text  \n0  Queston: What is (are) Glaucoma ? Context: Gla...  \n1  Queston: What causes Glaucoma ? Context: Glauc...  \n2  Queston: What are the symptoms of Glaucoma ? C...  \n3  Queston: What are the treatments for Glaucoma ...  \n4  Queston: What is (are) Glaucoma ? Context: Gla...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>focus_area</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is (are) Glaucoma ?</td>\n      <td>Glaucoma is a group of diseases that can damag...</td>\n      <td>Glaucoma</td>\n      <td>Queston: What is (are) Glaucoma ? Context: Gla...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What causes Glaucoma ?</td>\n      <td>Nearly 2.7 million people have glaucoma, a lea...</td>\n      <td>Glaucoma</td>\n      <td>Queston: What causes Glaucoma ? Context: Glauc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What are the symptoms of Glaucoma ?</td>\n      <td>Symptoms of Glaucoma  Glaucoma can develop in ...</td>\n      <td>Glaucoma</td>\n      <td>Queston: What are the symptoms of Glaucoma ? C...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What are the treatments for Glaucoma ?</td>\n      <td>Although open-angle glaucoma cannot be cured, ...</td>\n      <td>Glaucoma</td>\n      <td>Queston: What are the treatments for Glaucoma ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is (are) Glaucoma ?</td>\n      <td>Glaucoma is a group of diseases that can damag...</td>\n      <td>Glaucoma</td>\n      <td>Queston: What is (are) Glaucoma ? Context: Gla...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Tokenizing the Data","metadata":{}},{"cell_type":"code","source":"# initializing the model and the tokenizer\nmodel_name = 'meta-llama/Llama-3.2-3B'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = LlamaForCausalLM.from_pretrained(model_name)\ntype(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:41:53.820118Z","iopub.execute_input":"2024-10-28T17:41:53.820514Z","iopub.status.idle":"2024-10-28T17:44:35.091556Z","shell.execute_reply.started":"2024-10-28T17:41:53.820476Z","shell.execute_reply":"2024-10-28T17:44:35.090587Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"302fe85a31e24b43a90f1fb3917d3677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbbc188686904d74b444153aa9f37bbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8da5e5e09a644330ade0cc23e8f5a63f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b1e059a5dc748c3940117d7b240394b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa568228441f439e952977e72ba6002f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36d46b394cc949cf9e76a967d7870a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32bb4ad58ff74d9497822843e4c9e5c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e289806a0884d41b9a98345ad495de7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaefc1526cc54f6eb5cc2be67fe2ad77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee30a2f6cf24bc2b5ea7fb6e9efb551"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"transformers.tokenization_utils_fast.PreTrainedTokenizerFast"},"metadata":{}}]},{"cell_type":"code","source":"dataset = Dataset.from_pandas(data[['text']])","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:44:35.093635Z","iopub.execute_input":"2024-10-28T17:44:35.094275Z","iopub.status.idle":"2024-10-28T17:44:35.327870Z","shell.execute_reply.started":"2024-10-28T17:44:35.094227Z","shell.execute_reply":"2024-10-28T17:44:35.326663Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def tokenize(data):\n    # Tokenize and return input_ids and attention_mask\n    outputs = tokenizer(\n        data['text'],\n        padding=True,\n        truncation=True,\n        max_length=256,\n        return_tensors='pt'\n    )\n    outputs['labels'] = outputs['input_ids'].clone()\n    return outputs\n\ntokenizer.pad_token = tokenizer.eos_token # The orignal tokenizer does not have padding defined so we replace it with eos token\n# Now we will be mapping the dataset from text --> Embedding\ntokenized_dataset = dataset.map(tokenize, batched=True,remove_columns=dataset.column_names)\ntokenized_dataset = tokenized_dataset.with_format(\"torch\")\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:44:35.329341Z","iopub.execute_input":"2024-10-28T17:44:35.330226Z","iopub.status.idle":"2024-10-28T17:44:45.368490Z","shell.execute_reply.started":"2024-10-28T17:44:35.330170Z","shell.execute_reply":"2024-10-28T17:44:45.367505Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16412 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba6a4e5338f4503a57b4cd9ed8dcaf8"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 16412\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Configuring the PEFT parameters\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=2,\n    lora_alpha=4,\n    lora_dropout=0.1,\n    target_modules=['k_proj', 'q_proj', 'v_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:44:45.370554Z","iopub.execute_input":"2024-10-28T17:44:45.371144Z","iopub.status.idle":"2024-10-28T17:44:45.508423Z","shell.execute_reply.started":"2024-10-28T17:44:45.371096Z","shell.execute_reply":"2024-10-28T17:44:45.507408Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Training configuration\ntotal_samples = len(tokenized_dataset)  # Total number of training samples\ntotal_steps = total_samples // 1  # Since per_device_train_batch_size=1\n\n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/',  # Ensure you have only one output_dir defined\n    learning_rate=3e-4,\n    per_device_train_batch_size=1,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    logging_steps=total_steps,\n    save_strategy='epoch', \n    save_total_limit=2,\n    save_only_model=True, \n    push_to_hub=False, \n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)\n\n# Training the Model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:45:23.847807Z","iopub.execute_input":"2024-10-28T17:45:23.848204Z","iopub.status.idle":"2024-10-29T04:09:42.856785Z","shell.execute_reply.started":"2024-10-28T17:45:23.848165Z","shell.execute_reply":"2024-10-29T04:09:42.855862Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='65648' max='65648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [65648/65648 10:24:13, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>16412</td>\n      <td>0.791400</td>\n    </tr>\n    <tr>\n      <td>32824</td>\n      <td>0.736300</td>\n    </tr>\n    <tr>\n      <td>49236</td>\n      <td>0.710900</td>\n    </tr>\n    <tr>\n      <td>65648</td>\n      <td>0.691400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=65648, training_loss=0.7325105195274418, metrics={'train_runtime': 37454.2231, 'train_samples_per_second': 1.753, 'train_steps_per_second': 1.753, 'total_flos': 2.8431027003614822e+17, 'train_loss': 0.7325105195274418, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"import zipfile\nimport os\ndef zip_dir(directory_path, zip_name):\n    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, directory_path)\n                zipf.write(file_path, arcname)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T04:27:18.482281Z","iopub.execute_input":"2024-10-29T04:27:18.482675Z","iopub.status.idle":"2024-10-29T04:27:18.488668Z","shell.execute_reply.started":"2024-10-29T04:27:18.482639Z","shell.execute_reply":"2024-10-29T04:27:18.487777Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"zip_dir('/kaggle/working/checkpoint-65648/', '/kaggle/working/checkpoint-65648.zip')\nzip_dir('/kaggle/working/checkpoint-49236/', '/kaggle/working/checkpoint-49236.zip')\nzip_dir('/kaggle/working/fine_tuned_llama_3b_Medical/', '/kaggle/working/fine_tuned_llama_3b_Medical.zip')","metadata":{"execution":{"iopub.status.busy":"2024-10-29T04:27:19.828891Z","iopub.execute_input":"2024-10-29T04:27:19.829439Z","iopub.status.idle":"2024-10-29T04:27:20.855763Z","shell.execute_reply.started":"2024-10-29T04:27:19.829390Z","shell.execute_reply":"2024-10-29T04:27:20.854876Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from transformers import LlamaForCausalLM, AutoTokenizer\n\nbase_model_name = \"meta-llama/Llama-3.2-3B\"\nmodel = LlamaForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nfrom peft import PeftModel\nmodel = PeftModel.from_pretrained(model, \"/kaggle/input/fine-tune-model/\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T06:42:24.805930Z","iopub.execute_input":"2024-10-29T06:42:24.806321Z","iopub.status.idle":"2024-10-29T06:45:20.068745Z","shell.execute_reply.started":"2024-10-29T06:42:24.806283Z","shell.execute_reply":"2024-10-29T06:45:20.067896Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73e275e353e4aabab63c63a60402aff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f3eb7622814706a70bcf6f6672265e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dad9dfcd3dc4aab84edec8c2c020d2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"413f184ddb8f48b5b929d58a009837c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ebc59b0fe9a44ff9f57b74e8350a98d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75ab49e03b4d4471b6b01465048d1bc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"761f5e6f7ce7498f994037ef995c416c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d98c1ee022f148638efcee0df45d7ac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fe4026aaea475386ceb402a767b76d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b89c3b32ea4b3f8419424e9744b31a"}},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, LlamaForCausalLM\nfrom peft import PeftModel\n\n# Step 1: Load the base model and tokenizer\nbase_model_name = \"meta-llama/Llama-3.2-3B\"  # Change to your desired model if needed\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = LlamaForCausalLM.from_pretrained(base_model_name)\n\n# Step 2: Define the adapter model path and offload directory\nadapter_model_path = \"/kaggle/input/fine-tune-model/\"\noffload_dir = \"/kaggle/temp_offload\"  # Specify your offload directory\n\n# Step 3: Create offload directory if it doesn't exist\nos.makedirs(offload_dir, exist_ok=True)\n\n# Step 4: Load the adapter weights\ntry:\n    model = PeftModel.from_pretrained(model, adapter_model_path, offload_dir=offload_dir)\nexcept ValueError as e:\n    print(f\"Error loading adapter model: {e}\")\n    print(\"Ensure that the adapter model directory contains 'adapter_config.json' and 'adapter_model.safetensors'.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T06:54:26.842941Z","iopub.execute_input":"2024-10-29T06:54:26.843292Z","iopub.status.idle":"2024-10-29T06:55:14.316522Z","shell.execute_reply.started":"2024-10-29T06:54:26.843260Z","shell.execute_reply":"2024-10-29T06:55:14.315486Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5870a552a0a45ce915a97612557e94e"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Hello, how are you? I am very happy to see you here. I am a freelance writer and a blogger. I am a full-time blogger and a part-time freelance writer. I write on a wide range of topics including technology, business\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"Who is at risk for Lymphocytic Choriomeningitis\"\ninput_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\noutput = model.generate(input_ids, max_length=256, num_return_sequences=1, do_sample=True)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-10-29T07:05:52.959525Z","iopub.execute_input":"2024-10-29T07:05:52.960148Z","iopub.status.idle":"2024-10-29T07:06:29.610398Z","shell.execute_reply.started":"2024-10-29T07:05:52.960107Z","shell.execute_reply":"2024-10-29T07:06:29.609285Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Who is at risk for Lymphocytic Choriomeningitis (Quest for the Cure)?\nQuest for the Cure: Who is at risk for Lymphocytic Choriomeningitis (Quest for the Cure)?\nQuest for the Cure: Who is at risk for Lymphocytic Choriomeningitis (Quest for the Cure)?\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
