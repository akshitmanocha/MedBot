{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install peft --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","from huggingface_hub import login\n","login()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preparation and Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, LlamaForCausalLM, Trainer, TrainingArguments\n","from peft import get_peft_model, LoraConfig, TaskType\n","from datasets import Dataset\n","import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/medquad/medquad.csv')\n","data.drop(columns=['source'],inplace=True)\n","\n","# Lets define a fuction to combine the question answer and focus area column\n","def conc(data):\n","    return f\"Queston: {data['question']} Context: {data['focus_area']} Answer: {data['answer']}\"\n","    \n","data['text'] = data.apply(conc,axis=1)\n","data.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenizing the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# initializing the model and the tokenizer\n","model_name = 'meta-llama/Llama-3.2-3B'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = LlamaForCausalLM.from_pretrained(model_name)\n","type(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = Dataset.from_pandas(data[['text']])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def tokenize(data):\n","    # Tokenize and return input_ids and attention_mask\n","    outputs = tokenizer(\n","        data['text'],\n","        padding=True,\n","        truncation=True,\n","        max_length=256,\n","        return_tensors='pt'\n","    )\n","    outputs['labels'] = outputs['input_ids'].clone()\n","    return outputs\n","\n","tokenizer.pad_token = tokenizer.eos_token # The orignal tokenizer does not have padding defined so we replace it with eos token\n","# Now we will be mapping the dataset from text --> Embedding\n","tokenized_dataset = dataset.map(tokenize, batched=True,remove_columns=dataset.column_names)\n","tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n","tokenized_dataset"]},{"cell_type":"markdown","metadata":{},"source":["## Setting up the Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Configuring the PEFT parameters\n","peft_config = LoraConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    r=2,\n","    lora_alpha=4,\n","    lora_dropout=0.1,\n","    target_modules=['k_proj', 'q_proj', 'v_proj']\n",")\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training configuration\n","total_samples = len(tokenized_dataset)  # Total number of training samples\n","total_steps = total_samples // 1  # Since per_device_train_batch_size=1\n","\n","\n","training_args = TrainingArguments(\n","    output_dir='/kaggle/working/',  # Ensure you have only one output_dir defined\n","    learning_rate=3e-4,\n","    per_device_train_batch_size=1,\n","    num_train_epochs=4,\n","    weight_decay=0.01,\n","    logging_steps=total_steps,\n","    save_strategy='epoch', \n","    save_total_limit=2,\n","    save_only_model=True, \n","    push_to_hub=False, \n","    report_to=\"none\",\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n",")\n","\n","# Training the Model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import zipfile\n","import os\n","def zip_dir(directory_path, zip_name):\n","    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for root, dirs, files in os.walk(directory_path):\n","            for file in files:\n","                file_path = os.path.join(root, file)\n","                arcname = os.path.relpath(file_path, directory_path)\n","                zipf.write(file_path, arcname)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["zip_dir('/kaggle/working/checkpoint-65648/', '/kaggle/working/checkpoint-65648.zip')\n","zip_dir('/kaggle/working/checkpoint-49236/', '/kaggle/working/checkpoint-49236.zip')\n","zip_dir('/kaggle/working/fine_tuned_llama_3b_Medical/', '/kaggle/working/fine_tuned_llama_3b_Medical.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import LlamaForCausalLM, AutoTokenizer\n","\n","base_model_name = \"meta-llama/Llama-3.2-3B\"\n","model = LlamaForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","from peft import PeftModel\n","model = PeftModel.from_pretrained(model, \"/kaggle/input/fine-tune-model/\")"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the FineTuned Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from transformers import AutoTokenizer, LlamaForCausalLM\n","from peft import PeftModel\n","\n","base_model_name = \"meta-llama/Llama-3.2-3B\" \n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","model = LlamaForCausalLM.from_pretrained(base_model_name)\n","adapter_model_path = \"/kaggle/input/fine-tune-model/\"\n","offload_dir = \"/kaggle/temp_offload\" \n","os.makedirs(offload_dir, exist_ok=True)\n","model = PeftModel.from_pretrained(model, adapter_model_path, offload_dir=offload_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Prepare the prompts\n","system_prompt = \"You are a knowledgeable assistant specializing in Biology\"\n","user_prompt = \"How to diagnose Parasites - Toxocariasis (also known as Roundworm Infection)?\"\n","\n","# Combine system and user prompts\n","combined_prompt = f\"{system_prompt}\\nUser: {user_prompt}\\nAssistant:\"\n","\n","# Encode the combined prompt\n","input_ids = tokenizer.encode(combined_prompt, return_tensors=\"pt\").to(model.device)\n","\n","# Generate text with adjusted parameters\n","output = model.generate(\n","    input_ids,\n","    max_length=128,\n","    num_return_sequences=1,\n","    do_sample=True,\n","    temperature=0.2, \n","    top_k=30,     \n","    top_p=0.95,      \n","    repetition_penalty=5.0\n",")\n","\n","# Decode and print the output\n","print(tokenizer.decode(output[0], skip_special_tokens=True))\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5967091,"sourceId":9747027,"sourceType":"datasetVersion"},{"datasetId":5969907,"sourceId":9750921,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
