{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9747027,"sourceType":"datasetVersion","datasetId":5967091},{"sourceId":9750921,"sourceType":"datasetVersion","datasetId":5969907}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft --quiet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n\n# Set your API token as an environment variable\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"  \n\n# Login using the environment variable\nlogin(token=os.environ[\"HUGGINGFACE_TOKEN\"], write_permission=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Prepration and Tokenization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, LlamaForCausalLM, Trainer, TrainingArguments\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import Dataset\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/medquad/medquad.csv')\ndata.drop(columns=['source'],inplace=True)\n\n# Lets define a fuction to combine the question answer and focus area column\ndef conc(data):\n    return f\"Queston: {data['question']} Context: {data['focus_area']} Answer: {data['answer']}\"\n    \ndata['text'] = data.apply(conc,axis=1)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenizing the Data","metadata":{}},{"cell_type":"code","source":"# initializing the model and the tokenizer\nmodel_name = 'meta-llama/Llama-3.2-3B'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = LlamaForCausalLM.from_pretrained(model_name)\ntype(tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset.from_pandas(data[['text']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(data):\n    # Tokenize and return input_ids and attention_mask\n    outputs = tokenizer(\n        data['text'],\n        padding=True,\n        truncation=True,\n        max_length=256,\n        return_tensors='pt'\n    )\n    outputs['labels'] = outputs['input_ids'].clone()\n    return outputs\n\ntokenizer.pad_token = tokenizer.eos_token # The orignal tokenizer does not have padding defined so we replace it with eos token\n# Now we will be mapping the dataset from text --> Embedding\ntokenized_dataset = dataset.map(tokenize, batched=True,remove_columns=dataset.column_names)\ntokenized_dataset = tokenized_dataset.with_format(\"torch\")\ntokenized_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuring the PEFT parameters\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=2,\n    lora_alpha=4,\n    lora_dropout=0.1,\n    target_modules=['k_proj', 'q_proj', 'v_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training configuration\ntotal_samples = len(tokenized_dataset)  # Total number of training samples\ntotal_steps = total_samples // 1  # Since per_device_train_batch_size=1\n\n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/',  # Ensure you have only one output_dir defined\n    learning_rate=3e-4,\n    per_device_train_batch_size=1,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    logging_steps=total_steps,\n    save_strategy='epoch', \n    save_total_limit=2,\n    save_only_model=True, \n    push_to_hub=False, \n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)\n\n# Training the Model\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\ndef zip_dir(directory_path, zip_name):\n    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, directory_path)\n                zipf.write(file_path, arcname)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_dir('/kaggle/working/checkpoint-65648/', '/kaggle/working/checkpoint-65648.zip')\nzip_dir('/kaggle/working/checkpoint-49236/', '/kaggle/working/checkpoint-49236.zip')\nzip_dir('/kaggle/working/fine_tuned_llama_3b_Medical/', '/kaggle/working/fine_tuned_llama_3b_Medical.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LlamaForCausalLM, AutoTokenizer\n\nbase_model_name = \"meta-llama/Llama-3.2-3B\"\nmodel = LlamaForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nfrom peft import PeftModel\nmodel = PeftModel.from_pretrained(model, \"/kaggle/input/fine-tune-model/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Pretrained Model","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, LlamaForCausalLM\nfrom peft import PeftModel\n\nbase_model_name = \"meta-llama/Llama-3.2-3B\" \ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = LlamaForCausalLM.from_pretrained(base_model_name)\nadapter_model_path = \"/kaggle/input/fine-tune-model/\"\noffload_dir = \"/kaggle/temp_offload\" \nos.makedirs(offload_dir, exist_ok=True)\nmodel = PeftModel.from_pretrained(model, adapter_model_path, offload_dir=offload_dir)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the prompts\nsystem_prompt = \"You are a knowledgeable assistant specializing in Biology\"\nuser_prompt = \"How to diagnose Parasites - Toxocariasis (also known as Roundworm Infection)?\"\n\n# Combine system and user prompts\ncombined_prompt = f\"{system_prompt}\\nUser: {user_prompt}\\nAssistant:\"\n\n# Encode the combined prompt\ninput_ids = tokenizer.encode(combined_prompt, return_tensors=\"pt\").to(model.device)\n\n# Generate text with adjusted parameters\noutput = model.generate(\n    input_ids,\n    max_length=128,\n    num_return_sequences=1,\n    do_sample=True,\n    temperature=0.2, \n    top_k=30,     \n    top_p=0.95,      \n    repetition_penalty=5.0\n)\n\n# Decode and print the output\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}