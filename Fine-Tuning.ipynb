{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    datasets \\\n",
    "    evaluate \\\n",
    "    rouge_score\\\n",
    "    loralib \\\n",
    "    evaluate \\\n",
    "    accelerate \\\n",
    "    bitsandbytes \\\n",
    "    trl \\\n",
    "    peft \\\n",
    "    -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, Trainer, TrainingArguments,BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  \n",
    "    bnb_8bit_compute_dtype=compute_dtype, \n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, quantization_config=quant_config,torch_dtype=compute_dtype)\n",
    "print(type(tokenizer))\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/medquad/medquad.csv')\n",
    "data.drop(columns=['source'], inplace=True)\n",
    "def conc(data):\n",
    "    return f\"{data['question']} ({data['focus_area']})\"\n",
    "\n",
    "data['question'] = data.apply(conc, axis=1)\n",
    "data.drop(columns=['focus_area'], inplace=True)\n",
    "\n",
    "data['answer_word_count'] = data['answer'].apply(lambda x: len(str(x).split()))\n",
    "data = data[data['answer_word_count'] <= 512].drop(columns=['answer_word_count'])\n",
    "\n",
    "def tokenize_function(row):\n",
    "    question = str(row['question'])\n",
    "    answer = str(row['answer']) \n",
    "    \n",
    "    row['input_ids'] = tokenizer(question, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").input_ids[0]\n",
    "    row['labels'] = tokenizer(answer, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").input_ids[0]\n",
    "    \n",
    "    return row\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenized_data = data.apply(tokenize_function, axis=1)\n",
    "\n",
    "tokenized_data['input_ids'] = tokenized_data['input_ids'].apply(lambda x: x.tolist())\n",
    "tokenized_data['labels'] = tokenized_data['labels'].apply(lambda x: x.tolist())\n",
    "tokenized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(tokenized_data)\n",
    "tokenized_datasets = dataset.map(tokenize_function)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['question', '__index_level_0__','answer'])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, TaskType\n",
    "# Configuring the PEFT parameters\n",
    "peft_args = LoraConfig(\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_args)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./Output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the FineTuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(base_model_name)\n",
    "adapter_model_path = \"/kaggle/input/fine-tune-model/\"\n",
    "offload_dir = \"/kaggle/temp_offload\" \n",
    "os.makedirs(offload_dir, exist_ok=True)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter_model_path, offload_dir=offload_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine system and user prompts\n",
    "prompt = \"Who is at risk for Alkhurma Hemorrhagic Fever (AHF)\"\n",
    "\n",
    "# Encode the combined prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate text with adjusted parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=64,\n",
    "    num_return_sequences=1,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,   \n",
    "    top_p=0.01,      \n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5967091,
     "sourceId": 9747027,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
