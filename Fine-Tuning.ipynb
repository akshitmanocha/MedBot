{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9747027,"sourceType":"datasetVersion","datasetId":5967091}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install \\\n    datasets \\\n    evaluate \\\n    rouge_score\\\n    loralib \\\n    evaluate \\\n    accelerate \\\n    bitsandbytes \\\n    trl \\\n    peft \\\n    -U --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, LlamaForCausalLM, Trainer, TrainingArguments,BitsAndBytesConfig\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\nfrom datasets import Dataset\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = 'meta-llama/Llama-3.2-3B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ncompute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_8bit=True,  \n    bnb_8bit_compute_dtype=compute_dtype, \n)\n\nmodel = LlamaForCausalLM.from_pretrained(model_name, quantization_config=quant_config,torch_dtype=compute_dtype)\nprint(type(tokenizer))\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/medquad/medquad.csv')\ndata.drop(columns=['source'], inplace=True)\ndef conc(data):\n    return f\"{data['question']} ({data['focus_area']})\"\n\ndata['question'] = data.apply(conc, axis=1)\ndata.drop(columns=['focus_area'], inplace=True)\n\ndata['answer_word_count'] = data['answer'].apply(lambda x: len(str(x).split()))\ndata = data[data['answer_word_count'] <= 512].drop(columns=['answer_word_count'])\n\ndef tokenize_function(row):\n    question = str(row['question'])\n    answer = str(row['answer']) \n    \n    row['input_ids'] = tokenizer(question, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").input_ids[0]\n    row['labels'] = tokenizer(answer, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").input_ids[0]\n    \n    return row\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\ntokenized_data = data.apply(tokenize_function, axis=1)\n\ntokenized_data['input_ids'] = tokenized_data['input_ids'].apply(lambda x: x.tolist())\ntokenized_data['labels'] = tokenized_data['labels'].apply(lambda x: x.tolist())\ntokenized_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = Dataset.from_pandas(tokenized_data)\ntokenized_datasets = dataset.map(tokenize_function)\ntokenized_datasets = tokenized_datasets.remove_columns(['question', '__index_level_0__','answer'])\ntokenized_datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import get_peft_model, TaskType\n# Configuring the PEFT parameters\npeft_args = LoraConfig(\n    lora_alpha=64,\n    lora_dropout=0.1,\n    r=32,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\npeft_model = get_peft_model(model, peft_args)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set training parameters\ntraining_params = TrainingArguments(\n    output_dir=\"./Output\",\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=1000,\n    logging_steps=1000,\n    learning_rate=2e-5,\n    weight_decay=0.001,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    logging_dir=\"./logs\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=dataset,\n    peft_config=peft_args,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n    tokenizer=tokenizer,\n    args=training_params,\n    packing=False,\n)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the FineTuned Model","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, LlamaForCausalLM\nfrom peft import PeftModel\n\nbase_model_name = \"meta-llama/Llama-3.2-3B\" \ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = LlamaForCausalLM.from_pretrained(base_model_name)\nadapter_model_path = \"/kaggle/input/fine-tune-model/\"\noffload_dir = \"/kaggle/temp_offload\" \nos.makedirs(offload_dir, exist_ok=True)\n\nmodel = PeftModel.from_pretrained(model, adapter_model_path, offload_dir=offload_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine system and user prompts\nprompt = \"Who is at risk for Alkhurma Hemorrhagic Fever (AHF)\"\n\n# Encode the combined prompt\ninput_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n\n# Generate text with adjusted parameters\noutput = model.generate(\n    input_ids,\n    max_length=64,\n    num_return_sequences=1,\n    use_cache=True,\n    temperature=1.5,   \n    top_p=0.01,      \n)\n\n# Decode and print the output\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}